import tensorflow as tf
import os
import scipy.misc
import pandas as pd
import numpy as np
from pyduyp.logger.log import log
from pyduyp.utils.dl.ops.dlops import *


def VGG19(x, is_training, reuse=False):
    with tf.variable_scope('vgg19', reuse=reuse):
        phi = []
        with tf.variable_scope('conv1a'):
            x = conv_layer(x, [3, 3, 3, 64], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        with tf.variable_scope('conv1b'):
            x = conv_layer(x, [3, 3, 64, 64], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        phi.append(x)

        x = max_pooling_layer(x, 2, 2)
        with tf.variable_scope('conv2a'):
            x = conv_layer(x, [3, 3, 64, 128], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        with tf.variable_scope('conv2b'):
            x = conv_layer(x, [3, 3, 128, 128], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        phi.append(x)

        x = max_pooling_layer(x, 2, 2)
        with tf.variable_scope('conv3a'):
            x = conv_layer(x, [3, 3, 128, 256], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        with tf.variable_scope('conv3b'):
            x = conv_layer(x, [3, 3, 256, 256], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        with tf.variable_scope('conv3c'):
            x = conv_layer(x, [3, 3, 256, 256], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        with tf.variable_scope('conv3d'):
            x = conv_layer(x, [3, 3, 256, 256], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        phi.append(x)

        x = max_pooling_layer(x, 2, 2)
        with tf.variable_scope('conv4a'):
            x = conv_layer(x, [3, 3, 256, 512], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        with tf.variable_scope('conv4b'):
            x = conv_layer(x, [3, 3, 512, 512], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        with tf.variable_scope('conv4c'):
            x = conv_layer(x, [3, 3, 512, 512], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        with tf.variable_scope('conv4d'):
            x = conv_layer(x, [3, 3, 512, 512], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        phi.append(x)

        x = max_pooling_layer(x, 2, 2)
        with tf.variable_scope('conv5a'):
            x = conv_layer(x, [3, 3, 512, 512], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        with tf.variable_scope('conv5b'):
            x = conv_layer(x, [3, 3, 512, 512], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        with tf.variable_scope('conv5c'):
            x = conv_layer(x, [3, 3, 512, 512], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        with tf.variable_scope('conv5d'):
            x = conv_layer(x, [3, 3, 512, 512], 1)
            x = batch_normalize(x, is_training)
            x = lrelu(x)
        phi.append(x)

        x = max_pooling_layer(x, 2, 2)
        x = flatten_layer(x)
        with tf.variable_scope('fc1'):
            x = full_connection_layer(x, 4096)
            x = lrelu(x)
        with tf.variable_scope('fc2'):
            x = full_connection_layer(x, 4096)
            x = lrelu(x)
        with tf.variable_scope('softmax'):
            x = full_connection_layer(x, 300)

        return x



def cosdistance(vector1, vector2):
    """
    ex:
        print(cosdistance([1, 2, 3], [3, 4, 5]))

    :param vector1:
    :param vector2:
    :return:
    """
    dot_product = 0.0
    normA = 0.0
    normB = 0.0
    for a, b in zip(vector1, vector2):
        dot_product += a * b
        normA += a ** 2
        normB += b ** 2
    if normA == 0.0 or normB == 0.0:
        return None
    else:
        return dot_product / ((normA * normB) ** 0.5)

str1 = '空调送风千篇一律 卡萨帝CES展示1台空调吹多种温度 智慧家庭将成为2018家居家电领域新风口。美国当地时间1月9日,第51届国际消费类电子产品展览会(简称CES)在美国拉斯维加斯开幕,' \
           '卡萨帝全球首台智慧多温区家用空调——天玺空调在智慧家庭场景体验区亮相。其作为行业首个突破“空调一次只能吹一种温度的风”智慧多温区家用空调,不仅引领行业进入“定制送风”时代,' \
           '更为全球用户重新发掘智慧家庭“定制”体验。 千篇一律 行业百年只吹一种风 随着消费升级已成共识,家电市场正在由基础性消费向品质型消费升级,' \
           '直接表现为愿为高端空调买单的用户比例提高。据《高端家电产品消费者调查报告》显示,有高端家电购买计划的人群收入高、对价格不敏感,替换型需求和提升生活品质成为他们的共性,' \
           '其中有69.9%的人群购买理由是为追求生活品质。 转而看高端空调领域,目前市场主流多为追求恒温、节能、智能等产品,但随着替换性需求越来越明显,现代高端家庭对空调需求更多表现为对多温区送风的诉求,' \
           '而传统空调一次只能吹一种温度的风,显然不能满足这种高端用户需求。比如,在同一室内的青年、儿童和老人,如果室内空调所出风力、温度等都一致,则可能导致青年感觉较热,老人和儿童感觉偏冷。针对此类问题,' \
           '卡萨帝深耕用户实际需求,首次终结了传统空调单一控温、无法定制送风是困扰用户已久的这一痛点,发明一次可吹出多种温度风的天玺空调,在此次CES展上,在智慧家庭体验区展出的卡萨帝天玺空调以 ' \
           '“定制送风”成为现场最受全球客商关注的智能家电。 逆向思维 多温区空调突破行业瓶颈 面对更高端、个性化的用户需求,全行业都为了得到更舒适的空气使用体验都在同一种送风模式的研发上大做文章,' \
           '此时卡萨帝却打破行业多年传统采用多温区送风的逆向思维模式。在今年这个集人工智能、物联网等新一代信息技术的舞台上,' \
           '卡萨帝天玺空调无论在技术构想、用户体验上都满足了用户对智能家居和高端体验的全面构想。来自纽约的参观者Williams是一名人工智能工程师,' \
           '在体验天玺空调的多温区送风后感叹:“没想到卡萨帝空调使空气都变得智能起来,这才应该是未来人类的真正需求。” Williams所指的真正需求就是卡萨帝天玺打造的的针对不同人群的定制送风。 在展会现场,' \
           '卡萨帝还邀请不同年龄的两位参展者亲身体验天玺空调的“定制送风”,年长的参展者表示感觉风力柔和温度舒适,年轻的参展者表示感觉风力舒适且出风凉爽;据了解,此智慧识别效果源自于卡萨帝智慧人体温冷感知系统,' \
           '其能智能检测人体、环境温度等参数,形成红外热图像,并通过大数据计算出人体主观舒适度从而获得室内人员的冷热感受,进而通过独立的双循环送风系统,根据不同人的体感需求实现“定制送风”。 ' \
           '近两年来全球正在悄悄燃起一场智能化革命,从此次CES展会上就可以看出,智能化已经深入到了人类社会的各个领域。而真正的智能应该是饱含人类智慧的人工替代者,具体落地在家电即是让用户随时体验所想,目前在空调领域,' \
           '卡萨帝坚守“人单合一”模式,让员工与用户融为一体,员工深度了解用户诉求,让用户直接产于到产品的研发,打造天玺空调以“定制风”颠覆传统空调对人体舒适度的调节标准。可以说,' \
           '天玺空调是卡萨帝“人单合一”模式中迭代出的代表性产品,为全人类智慧家庭提供更高端选择的同时,更为空调行业未来在智能领域的发展划定了方向。 '

vecots = '0.0016817 -0.0023771 0.0014308 0.0017345 0.00036935 -0.002784 -0.0011625 0.0013753 0.00041258 -0.0002469 -0.0006787 -0.00034368 0.0014742 0.0012733 -0.0020595 -0.00027367 -0.002166 0.0044171 0.0041103 0.0025185 6.2085e-06 -0.0013929 0.0028849 0.0045143 0.0018157 0.0034017 0.0010038 -0.0016331 -0.0020036 0.00040291 -0.0028853 0.00028505 -0.0035071 0.000594 -0.0075253 -0.0022002 0.00015122 -0.00096553 -0.006448 -0.0020763 -0.0011463 -0.0020786 -0.00045267 -0.00020631 0.0038685 0.002328 0.0029051 -0.0045597 0.0011096 -0.0031578 -0.00055255 7.3881e-05 -0.0010105 0.0041443 -0.0014362 -0.00074608 -0.0022296 0.0071468 0.00026954 0.0048568 0.0046115 0.0070269 0.0014227 0.0013971 0.00089506 0.00089894 -0.00015574 0.0033562 -0.0024287 0.0010246 -0.0033144 -0.0031511 -0.0033345 0.004288 -0.00052171 -0.00089313 0.0047684 -0.0010132 -0.0024283 -0.005237 -0.0036746 0.001436 0.0061823 -0.0055095 0.005296 -0.0035991 -0.00093066 0.0038196 0.00032246 -0.0039256 0.0072522 -0.0022805 0.0055718 -0.0035988 -0.0032735 -0.0014788 0.0024037 0.0026939 6.2561e-06 -0.0011473 0.0017805 -0.0029199 -2.6602e-05 -0.0051363 0.00049094 1.6564e-05 -0.0048527 0.0039031 0.0012706 -0.0042202 0.0026411 -0.0013751 -0.0040701 -0.0011444 -0.0026219 0.00075352 0.0012023 0.00225 -0.0028171 -0.00066409 -0.0005664 0.0040051 0.00075355 0.0010557 -0.0011771 -0.00097568 -0.00024623 0.0053235 0.0030382 -0.0017315 0.0024207 -0.0025765 0.0023139 -0.0032092 -0.003548 0.0022881 -0.0061837 0.00029765 0.00062331 0.00068086 -0.0027166 0.0031863 -0.0065737 0.0034293 0.0060602 0.004504 0.0053295 0.0024079 0.00070902 0.0028966 -0.0045681 0.0015634 -0.0014613 0.0069561 -0.0070769 0.0023017 -0.0011064 -0.00053212 0.0032405 -0.0035337 0.0026694 0.0035651 0.0047409 -0.0012295 -0.00032469 0.0025948 -0.0032963 0.00096035 -0.0043975 -0.0009855 -0.0035192 -0.00027234 -0.0030548 -0.0012547 -0.001251 -0.00011277 -0.0015221 -0.00045698 -0.00010189 -0.0018511 -0.0018755 -0.00048976 -0.0012928 0.001183 0.00031565 -0.00019164 -0.00089451 -0.00032064 -0.00039411 -0.0010326 -0.0011004 -0.00057266 -0.00022796 -0.0045244 0.0018536 3.0427e-05 0.001012 0.00053805 -0.0029383 0.0014684 -0.00035749 0.0012045 -0.0016012 -4.948e-05 -0.001409 -0.00033843 -0.0012289 -0.0013084 0.0019772 0.0028715 -0.0031593 -0.00010765 -0.0026751 -0.0010606 -0.0035321 0.0012284 0.0014916 -4.2898e-05 -0.00025318 0.00037125 -0.00074562 -0.00032884 0.00075165 0.0013371 0.00042652 -0.00058873 0.00061513 -0.00090333 -0.0022328 0.0010914 0.0002528 0.002822 -0.0016027 -1.5497e-05 -0.0014826 0.00015337 0.00052575 0.0024139 -0.0025401 0.0021926 -0.0013335 0.0019168 0.0030731 0.0024576 0.0012196 0.00036149 -0.00027879 0.0049065 0.00053997 0.0039811 -0.0024202 0.00069778 -0.0026459 -0.0019546 -0.0068449 0.0020488 0.00086165 -0.0018356 -0.0021088 0.0020403 0.0036333 -0.00074625 0.001589 0.0024911 -0.00043104 0.0016868 -0.0001082 0.00086151 -0.00018678 -0.0032375 0.0018129 -0.00011978 0.0058609 -0.00019446 -0.0026387 0.0016337 0.0039428 -0.00054639 0.0018987 -0.001241 0.0042978 0.0018963 -0.0015826 -0.0016058 -0.0006198 -0.00096858 0.0004066 0.0042636 0.0014089 -0.00094127 -0.001492 0.00016004 0.0027676 0.00023191 0.00017472 -0.00060181 0.00038404 0.000846 0.001399 0.0017932 '
v = []
for x in vecots.split():
    v.append(float(x))
vnew = np.array(v)

is_training = tf.placeholder(tf.bool, [])

image_holder = tf.placeholder(dtype=tf.float32, shape=[None, 256, 256, 3], name='input')
vgg = VGG19(image_holder, is_training)
with tf.Session() as sess:
    batch_size = 1
    imagename = '2.png'
    image = scipy.misc.imread(imagename)
    image = scipy.misc.imresize(image, (256, 256))

    image = image.reshape((batch_size, 256, 256, 3))
    init = tf.global_variables_initializer()
    sess.run(init)
    res = sess.run(vgg, feed_dict={image_holder: image, is_training: True})
    log.debug("{}, {}".format(vnew.shape, res[0].shape))
    print(cosdistance(vnew, res[0]))
